# Eureka MWE - Visual Architecture Guide

## System Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                         EUREKA MAIN LOOP                             │
│                        (eureka_loop.py)                              │
└────────────┬────────────────────────────────────────┬────────────────┘
             │                                        │
             ▼                                        ▼
    ┌────────────────┐                      ┌─────────────────┐
    │ Reward         │                      │ RL Trainer      │
    │ Generator      │◄────────────────────►│ (rl_trainer.py) │
    │ (reward_       │                      │                 │
    │  generator.py) │                      └────────┬────────┘
    └────────┬───────┘                               │
             │                                       │
             ▼                                       ▼
    ┌────────────────┐                      ┌─────────────────┐
    │ OpenAI API     │                      │ Stable-         │
    │ (GPT-3.5)      │                      │ Baselines3      │
    └────────────────┘                      │ (PPO)           │
                                            └────────┬────────┘
                                                     │
                                                     ▼
                                            ┌─────────────────┐
                                            │ Gymnasium       │
                                            │ (CartPole-v1)   │
                                            └─────────────────┘
```

---

## Data Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────┐
│                        EUREKA ITERATION                              │
└─────────────────────────────────────────────────────────────────────┘

STEP 1: REWARD GENERATION
──────────────────────────
Input:
  ├─ Environment code (CartPole)
  ├─ Task description
  └─ Previous performance (if iter > 0)
       │
       ▼
  [GPT-3.5 API Call]
       │
       ▼
Output:
  └─ 4 Reward Functions (Python code)


STEP 2: RL TRAINING (Parallel)
───────────────────────────────
For each reward function:
  │
  ├─ Load reward_0.py ──► Train PPO ──► Evaluate ──► Performance_0
  ├─ Load reward_1.py ──► Train PPO ──► Evaluate ──► Performance_1
  ├─ Load reward_2.py ──► Train PPO ──► Evaluate ──► Performance_2
  └─ Load reward_3.py ──► Train PPO ──► Evaluate ──► Performance_3


STEP 3: EVALUATION & RANKING
─────────────────────────────
  Performance_0: 145.2 ±  18.3
  Performance_1: 234.5 ±  12.1  ◄── Best!
  Performance_2: 178.9 ±  22.4
  Performance_3: 156.3 ±  19.8
       │
       ▼
  Select top 2 for next iteration


STEP 4: REFLECTION
──────────────────
Create summary:
  "Reward 1 performed best (234.5).
   It used a combination of:
   - Small angle penalty
   - Velocity term
   - Position bonus
   
   Reward 0 had harsh penalties.
   Reward 2 was too complex.
   ..."
       │
       ▼
Feed to LLM for next iteration
```

---

## File Interaction Map

```
                    ┌──────────────┐
                    │  config.py   │ (Parameters)
                    └──────┬───────┘
                           │ imported by
        ┌──────────────────┼──────────────────┐
        │                  │                  │
        ▼                  ▼                  ▼
┌───────────────┐  ┌──────────────┐  ┌──────────────┐
│ reward_       │  │ rl_trainer.py│  │ eureka_      │
│ generator.py  │  │              │  │ loop.py      │
└───────┬───────┘  └──────┬───────┘  └──────┬───────┘
        │                 │                  │
        │                 │                  │
        └─────────────────┴──────────────────┘
                          │
                          ▼
                  ┌──────────────┐
                  │  utils.py    │ (Plotting, logging)
                  └──────────────┘


Generated Files:
────────────────
rewards/
  ├─ iteration_0/
  │   ├─ reward_0.py ◄─┐
  │   ├─ reward_1.py   │ Generated by
  │   ├─ reward_2.py   │ reward_generator.py
  │   └─ reward_3.py ◄─┘
  └─ iteration_1/...

results/
  ├─ eureka_results.json ◄─┐
  ├─ summary.md            │ Generated by
  └─ plots/                │ eureka_loop.py
      ├─ eureka_results.png│ & utils.py
      └─ reward_dist.png ◄─┘
```

---

## Execution Timeline

```
TIME: 0:00 - Program Start
├─ [00:00] Initialize components
├─ [00:01] Test baseline rewards
│   ├─ Train sparse baseline (3 min)
│   └─ Train human baseline (3 min)
│
TIME: 0:06 - Iteration 1 Start
├─ [00:06] Generate 4 rewards via LLM (30 sec)
├─ [00:07] Train reward 0 (3 min)
├─ [00:10] Train reward 1 (3 min)
├─ [00:13] Train reward 2 (3 min)
├─ [00:16] Train reward 3 (3 min)
├─ [00:19] Evaluate and rank
│
TIME: 0:19 - Iteration 2 Start
├─ [00:19] Generate 4 improved rewards (30 sec)
├─ [00:20] Train 4 rewards (12 min)
├─ [00:32] Evaluate and rank
│
TIME: 0:32 - Iteration 3 Start
├─ [00:32] Generate 4 improved rewards (30 sec)
├─ [00:33] Train 4 rewards (12 min)
├─ [00:45] Evaluate and rank
│
TIME: 0:45 - Finalize
├─ [00:45] Generate plots
├─ [00:46] Save results
└─ [00:47] COMPLETE!

Total: ~47 minutes
```

---

## Code Flow Diagram

```
main()
  │
  ├─► EurekaLoop.__init__()
  │     ├─► RewardGenerator()
  │     ├─► RLTrainer()
  │     └─► create_output_dirs()
  │
  ├─► run_baseline_comparison()
  │     ├─► create_baseline_rewards()
  │     └─► rl_trainer.evaluate_reward() [x2]
  │
  └─► For each iteration:
        │
        ├─► run_iteration()
        │     │
        │     ├─► reward_generator.generate_rewards()
        │     │     ├─► create_initial_prompt() OR
        │     │     ├─► create_reflection_prompt()
        │     │     └─► OpenAI API call [x4]
        │     │
        │     └─► For each reward:
        │           │
        │           ├─► save_reward()
        │           └─► rl_trainer.train_with_reward()
        │                 ├─► load_reward_function()
        │                 ├─► create_env_with_reward()
        │                 ├─► PPO.learn()
        │                 └─► evaluate_policy()
        │
        ├─► plot_results()
        └─► save_final_results()
```

---

## Class & Function Map

```
reward_generator.py
└─ class RewardGenerator
    ├─ __init__(api_key)
    ├─ create_initial_prompt() → str
    ├─ create_reflection_prompt(prev_rewards) → str
    ├─ generate_rewards(n, prev_rewards) → List[str]
    ├─ _extract_code(response) → str
    ├─ _get_fallback_reward() → str
    └─ save_reward(code, iter, id) → path

rl_trainer.py
├─ class CustomRewardWrapper(Wrapper)
│   ├─ reset()
│   └─ step() → (obs, reward, done, info)
│
├─ class RLTrainer
│   ├─ __init__(env_name)
│   ├─ load_reward_function(code) → Callable
│   ├─ create_env_with_reward(fn) → gym.Env
│   ├─ train_with_reward(code) → (model, metrics)
│   └─ evaluate_reward(code) → metrics
│
└─ create_baseline_rewards() → Dict[str, str]

eureka_loop.py
└─ class EurekaLoop
    ├─ __init__(iterations, samples)
    ├─ run_baseline_comparison() → results
    ├─ run_iteration(iter, prev) → results
    ├─ run() → final_results
    ├─ print_final_summary()
    └─ save_final_results()

utils.py
├─ create_output_dirs(dir)
├─ plot_results(results, dir)
├─ plot_detailed_progression(results, dir)
├─ save_results(results, dir)
└─ create_summary_table(results, dir)
```

---

## State Machine Diagram

```
┌─────────────┐
│   INIT      │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│  BASELINE   │
│  EVAL       │
└──────┬──────┘
       │
       ▼
┌─────────────────────────────────────────┐
│         ITERATION LOOP                  │
│                                         │
│  ┌─────────────┐                       │
│  │  GENERATE   │                       │
│  │  REWARDS    │                       │
│  └──────┬──────┘                       │
│         │                               │
│         ▼                               │
│  ┌─────────────┐                       │
│  │   TRAIN     │ (4 parallel)          │
│  │   POLICIES  │                       │
│  └──────┬──────┘                       │
│         │                               │
│         ▼                               │
│  ┌─────────────┐                       │
│  │  EVALUATE   │                       │
│  │  & RANK     │                       │
│  └──────┬──────┘                       │
│         │                               │
│         ▼                               │
│  ┌─────────────┐                       │
│  │  REFLECT    │                       │
│  └──────┬──────┘                       │
│         │                               │
│         └───────► Next iteration        │
│                   or exit loop          │
└─────────────────────────────────────────┘
       │
       ▼
┌─────────────┐
│  FINALIZE   │
│  - Plot     │
│  - Save     │
│  - Report   │
└─────────────┘
```

---

## Memory Usage

```
Component                Memory Usage
────────────────────────────────────
Environment (CartPole)   < 1 MB
PPO Model                ~5-10 MB
Training Buffer          ~50 MB
Generated Rewards        < 1 MB
Results/Plots            < 10 MB
────────────────────────────────────
Total Peak Memory        ~100-200 MB
```

---

## API Call Pattern

```
Iteration 0:
  └─ 4 LLM calls (reward generation)
      ├─ Call 1: "Generate reward 0"
      ├─ Call 2: "Generate reward 1"
      ├─ Call 3: "Generate reward 2"
      └─ Call 4: "Generate reward 3"

Iteration 1:
  └─ 4 LLM calls (improvement)
      ├─ Call 5: "Improve based on prev results"
      ├─ Call 6: "..."
      ├─ Call 7: "..."
      └─ Call 8: "..."

Iteration 2:
  └─ 4 LLM calls (improvement)
      ├─ Call 9-12: "..."

Total: 12 LLM API calls
Cost: ~$0.12 (GPT-3.5) or ~$1.50 (GPT-4)
```

---

This visual guide should help you understand how all the pieces fit together!
